{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d0de5aa-5717-4b30-932c-d9b87bf35eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from transformers import AutoTokenizer\n",
    "import stanza\n",
    "from rapidfuzz import fuzz\n",
    "from collections import Counter\n",
    "#best working tokenizer\n",
    "nlp = stanza.Pipeline(\n",
    "    lang=\"nl\",\n",
    "    processors=\"tokenize\",\n",
    "    tokenize_no_ssplit=False,\n",
    "    tokenize_mwt=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6676644-cef5-4dd7-9669-081cb7905040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_tex.pkl\", \"rb\") as b:\n",
    "        link_tex=pickle.load(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4df47b5-9563-48d5-a953-ca4a56aa2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_link_text(t):\n",
    "    \"\"\"Return cleaned link text or None if invalid.\"\"\"\n",
    "    if t is None:\n",
    "        return None\n",
    "    stripped = t.strip()\n",
    "    if stripped == \"\" or all(ch in string.punctuation for ch in stripped):\n",
    "        return None\n",
    "    return stripped\n",
    "\n",
    "\n",
    "def extract_link_sentences(html: str,nlp,count,label: str = \"REFERENCE\"):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    container = soup.find(\"div\", {\"class\": \"tekst\"})\n",
    "    if not container:\n",
    "        return []\n",
    "\n",
    "    text = container.get_text(\" \")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sentences]\n",
    "\n",
    "    # ---- collect valid link texts ----\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", {\"class\": \"popuplink\"}):\n",
    "        link_text = clean_link_text(a.get_text(strip=True))\n",
    "        if link_text:\n",
    "            links.append(link_text)\n",
    "\n",
    "    if not links:\n",
    "        return [],[]\n",
    "    output_ref = []\n",
    "    output_not = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        raw_entities = []\n",
    "\n",
    "        # ---- collect all possible matches first ----\n",
    "        pos=[]\n",
    "        i=0\n",
    "        search_pos = 0  \n",
    "        for link in links:\n",
    "            # exact matches\n",
    "            pattern = re.compile(re.escape(link))\n",
    "            match = pattern.search(sent, search_pos)\n",
    "            if match:\n",
    "                raw_entities.append((match.start(), match.end(), label))\n",
    "                search_pos = match.end()  \n",
    "                pos.append(i)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        if not raw_entities:\n",
    "            output_not.append((sent, {\"entities\": []},count))\n",
    "            continue\n",
    "\n",
    "        # ---- enforce ordering rule ----\n",
    "        raw_entities.sort(key=lambda x: x[0])  # sort by start offset\n",
    "\n",
    "        filtered_entities = []\n",
    "        last_start = -1\n",
    "        k=0\n",
    "        d=0\n",
    "        for start, end, lbl in raw_entities:\n",
    "            if start > last_start:\n",
    "                filtered_entities.append((start, end, lbl))\n",
    "                last_start = start\n",
    "                links.pop(pos[k]-d)\n",
    "                d+=1\n",
    "            k+=1\n",
    "            # else: ignore match (starts before or at same position)\n",
    "\n",
    "        if filtered_entities:\n",
    "            output_ref.append((sent, {\"entities\": filtered_entities},count))\n",
    "    return output_ref,output_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64567871-083a-40aa-9ba6-495e066c5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need the html text\n",
    "texts=link_tex.values()\n",
    "#all found references\n",
    "training_data_ref=[]\n",
    "#all sentences without refrences\n",
    "training_data_neg=[]\n",
    "count=0\n",
    "\n",
    "for text in texts:\n",
    "    result_ref,result_neg=extract_link_sentences(text,nlp,count)\n",
    "    if result_ref != []:\n",
    "        training_data_ref.extend(result_ref)\n",
    "        training_data_neg.extend(result_neg)\n",
    "    else:\n",
    "        print(\"empty\")\n",
    "    count+=1\n",
    "    if count%500==0:\n",
    "        print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f01ab-7738-4389-86dd-db260a034864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open(\"training_data.pkl\", \"wb\") as b:\n",
    "        pickle.dump(training_data_ref, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a77a8-588c-45d1-a7a3-589ff4c5e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data spacy\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "\n",
    "INPUT_FILE = \"training_data.pkl\"\n",
    "TRAIN_OUT = \"train.spacy\"\n",
    "DEV_OUT = \"dev.spacy\"\n",
    "DEV_SPLIT = 0.1      \n",
    "NLP_MODEL = \"nl_core_news_sm\"   # tokenizer\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def create_docbin(nlp, data):\n",
    "    db = DocBin()\n",
    "    for text, ann,_ in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in ann[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data…\")\n",
    "    data = load_data(INPUT_FILE)\n",
    "\n",
    "    print(\"Shuffling and splitting…\")\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * (1 - DEV_SPLIT))\n",
    "    train_data = data[:split]\n",
    "    dev_data = data[split:]\n",
    "\n",
    "    print(f\"Train size: {len(train_data)}\")\n",
    "    print(f\"Dev size:   {len(dev_data)}\")\n",
    "\n",
    "    print(\"Loading spaCy tokenizer…\")\n",
    "    nlp = spacy.load(NLP_MODEL)\n",
    "\n",
    "    print(\"Converting training data → train.spacy…\")\n",
    "    train_db = create_docbin(nlp, train_data)\n",
    "    train_db.to_disk(TRAIN_OUT)\n",
    "\n",
    "    print(\"Converting dev data → dev.spacy…\")\n",
    "    dev_db = create_docbin(nlp, dev_data)\n",
    "    dev_db.to_disk(DEV_OUT)\n",
    "\n",
    "    print(\"\\nDone!\")\n",
    "    print(f\"Saved: {TRAIN_OUT}, {DEV_OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9e13-56f8-4ff0-942c-bb53ea801c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def merge(docs):\n",
    "    final=[]\n",
    "    highest=0\n",
    "    cursor=0\n",
    "    for doc in docs:\n",
    "        text,ent,doc_id= doc\n",
    "        if highest < doc_id:\n",
    "            highest= doc_id\n",
    "    list_text = [[] for _ in range(highest+1)]\n",
    "    entities = [[] for _ in range(highest+1)]\n",
    "    for doc in docs:\n",
    "        text,ent,doc_id= doc\n",
    "        list_text[doc_id].append(text)\n",
    "        entities[doc_id].append(ent)\n",
    "    for i in range(highest+1):\n",
    "        merged_text=\"\"\n",
    "        merged_entity=[]\n",
    "        for k in  range(len(list_text[i])):\n",
    "            sentence=list_text[i][k]\n",
    "            entity=entities[i][k]\n",
    "            for (s, e, lab) in entity.get(\"entities\", []):\n",
    "                merged_entity.append((cursor + s, cursor + e, lab))\n",
    "            merged_text= merged_text +sentence\n",
    "            cursor+= len(sentence)\n",
    "        \n",
    "        final.append((merged_text, {\"entities\": merged_entity}))\n",
    "        cursor=0\n",
    "    return final\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "Example = Tuple[str, Dict[str, Any]]  # (text, {\"entities\":[(start,end,label), ...]})\n",
    "LABEL = \"REFERENCE\"\n",
    "IN_TAG = \"I-REFERENCE\"\n",
    "OUT_TAG = \"O\"\n",
    "examples=training_data_ref+training_data_neg\n",
    "nlp = spacy.blank(\"nl\")\n",
    "def spans_to_io(doc, entities, label_name=LABEL):\n",
    "    \"\"\"\n",
    "    IO tagging: tokens are either inside a REFERENCE span (I-REFERENCE) or outside (O).\n",
    "    \"\"\"\n",
    "    spans = [(s, e) for (s, e, lab) in entities if lab == label_name]\n",
    "    tags = [OUT_TAG] * len(doc)\n",
    "\n",
    "    for i, tok in enumerate(doc):\n",
    "        ts, te = tok.idx, tok.idx + len(tok)\n",
    "        inside = any(ts < e and te > s for (s, e) in spans)  # any overlap with gold span\n",
    "        if inside:\n",
    "            tags[i] = IN_TAG\n",
    "    return tags\n",
    "\n",
    "def token_to_feature_text(doc, i: int) -> str:\n",
    "    tok = doc[i]\n",
    "    prev = doc[i-1] if i > 0 else None\n",
    "    nxt  = doc[i+1] if i < len(doc)-1 else None\n",
    "\n",
    "    parts = [\n",
    "        f\"w={tok.text.lower()}\",\n",
    "        f\"shape={tok.shape_}\",\n",
    "        f\"isupper={int(tok.is_upper)}\",\n",
    "        f\"istitle={int(tok.is_title)}\",\n",
    "        f\"isdigit={int(tok.is_digit)}\",\n",
    "        f\"pref3={tok.text[:3].lower()}\" if len(tok.text) >= 3 else f\"pref={tok.text.lower()}\",\n",
    "        f\"suf3={tok.text[-3:].lower()}\" if len(tok.text) >= 3 else f\"suf={tok.text.lower()}\",\n",
    "    ]\n",
    "\n",
    "    if prev:\n",
    "        parts += [f\"prev={prev.text.lower()}\", f\"prevshape={prev.shape_}\"]\n",
    "    else:\n",
    "        parts += [\"BOS=1\"]\n",
    "\n",
    "    if nxt:\n",
    "        parts += [f\"next={nxt.text.lower()}\", f\"nextshape={nxt.shape_}\"]\n",
    "    else:\n",
    "        parts += [\"EOS=1\"]\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def build_token_rows(docs: List[Example], nlp, batch_size=128):\n",
    "    \"\"\"\n",
    "    Build token-level dataset:\n",
    "      X: feature-string per token\n",
    "      y: IO tag per token\n",
    "    Also returns tokenized docs (so we can reconstruct spans later).\n",
    "    \"\"\"\n",
    "    X_text, y = [], []\n",
    "    texts = [t for (t, _) in docs]\n",
    "    anns  = [a for (_, a) in docs]\n",
    "\n",
    "    parsed_docs = []\n",
    "    for doc, ann in zip(nlp.pipe(texts, batch_size=batch_size), anns):\n",
    "        tags = spans_to_io(doc, ann.get(\"entities\", []))\n",
    "        parsed_docs.append((doc, tags, ann.get(\"entities\", [])))  # keep gold for eval\n",
    "        for i in range(len(doc)):\n",
    "            X_text.append(token_to_feature_text(doc, i))\n",
    "            y.append(tags[i])\n",
    "\n",
    "    return X_text, y, parsed_docs\n",
    "\n",
    "def io_tags_to_spans(doc, tags, label=LABEL):\n",
    "    \"\"\"\n",
    "    Convert consecutive I-REFERENCE tokens into character-level spans.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    start = None\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == IN_TAG and start is None:\n",
    "            start = doc[i].idx\n",
    "        if tag != IN_TAG and start is not None:\n",
    "            end = doc[i-1].idx + len(doc[i-1])\n",
    "            spans.append((start, end, label))\n",
    "            start = None\n",
    "\n",
    "    if start is not None:\n",
    "        end = doc[-1].idx + len(doc[-1])\n",
    "        spans.append((start, end, label))\n",
    "\n",
    "    return spans\n",
    "\n",
    "def span_f1_exact(gold_spans, pred_spans):\n",
    "    \"\"\"\n",
    "    Strict span match: (start,end,label) must match exactly.\n",
    "    \"\"\"\n",
    "    gold = set(gold_spans)\n",
    "    pred = set(pred_spans)\n",
    "\n",
    "    tp = len(gold & pred)\n",
    "    fp = len(pred - gold)\n",
    "    fn = len(gold - pred)\n",
    "\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "# -------------------------\n",
    "# YOUR INPUT HERE:\n",
    "# examples: List[Example] = ...\n",
    "# -------------------------\n",
    "print(\"part1\")\n",
    "docs_6600 = merge(examples)\n",
    "print(\"part2\")\n",
    "# Split by document to reduce leakage\n",
    "train_docs, test_docs = train_test_split(docs_6600, test_size=0.2, random_state=42)\n",
    "\n",
    "# Expand to token rows AFTER split (important)\n",
    "X_train, y_train, _ = build_token_rows(train_docs, nlp, batch_size=128)\n",
    "X_test,  y_test,  parsed_test = build_token_rows(test_docs,  nlp, batch_size=128)\n",
    "\n",
    "\n",
    "with open(\"LG_x_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(X_train, b)\n",
    "with open(\"LG_y_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(y_train, b)\n",
    "with open(\"LG_x_test.pkl\", \"wb\") as b:\n",
    "    pickle.dump(X_test, b)\n",
    "with open(\"LG_y_test.pkl\", \"wb\") as b:\n",
    "    pickle.dump(y_test, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006367a9-c5b2-4bf1-b23e-2800b2e19e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data roberta\n",
    "import pickle\n",
    "with open(\"training_data.pkl\", \"rb\") as f:\n",
    "    TRAIN_DATA=pickle.load(f)\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def preprocess_spacy_dataset(dataset, tokenizer, label2id, max_length=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for text, ann,_ in dataset:\n",
    "        entities = ann[\"entities\"]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        token_labels = [\"O\"] * len(offsets)\n",
    "\n",
    "        # Assign BIO labels\n",
    "        for start, end, ent_label in entities:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == tok_end:\n",
    "                    continue  # special tokens\n",
    "\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tok_start == start:\n",
    "                        token_labels[i] = f\"B-{ent_label}\"\n",
    "                    else:\n",
    "                        token_labels[i] = f\"I-{ent_label}\"\n",
    "\n",
    "        # Convert to label IDs\n",
    "        label_ids = []\n",
    "        for lab, (tok_start, tok_end) in zip(token_labels, offsets):\n",
    "            if tok_start == tok_end:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[lab])\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])\n",
    "        attention_masks.append(encoding[\"attention_mask\"])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "LABEL_LIST = [\"O\", \"B-REFERENCE\", \"I-REFERENCE\"]\n",
    "label2id = {l: i for i, l in enumerate(LABEL_LIST)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "MODEL_NAME = \"pdelobelle/robbert-v2-dutch-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "processed = preprocess_spacy_dataset(\n",
    "    TRAIN_DATA,\n",
    "    tokenizer,\n",
    "    label2id,\n",
    "    max_length=256\n",
    ")\n",
    "dataset = Dataset.from_dict(processed)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "with open(\"roberta_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(train_dataset, b)\n",
    "with open(\"roberta_test.pkl\", \"wb\") as b:\n",
    "        pickle.dump(eval_dataset, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d0df6-cf80-43db-abb8-ea7be98029cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
