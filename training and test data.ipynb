{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0de5aa-5717-4b30-932c-d9b87bf35eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from transformers import AutoTokenizer\n",
    "import stanza\n",
    "from rapidfuzz import fuzz\n",
    "from collections import Counter\n",
    "#best working tokenizer\n",
    "nlp = stanza.Pipeline(\n",
    "    lang=\"nl\",\n",
    "    processors=\"tokenize\",\n",
    "    tokenize_no_ssplit=False,\n",
    "    tokenize_mwt=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6676644-cef5-4dd7-9669-081cb7905040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_tex.pkl\", \"rb\") as b:\n",
    "        link_tex=pickle.load(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4df47b5-9563-48d5-a953-ca4a56aa2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_link_text(t):\n",
    "    \"\"\"Return cleaned link text or None if invalid.\"\"\"\n",
    "    if t is None:\n",
    "        return None\n",
    "    stripped = t.strip()\n",
    "    if stripped == \"\" or all(ch in string.punctuation for ch in stripped):\n",
    "        return None\n",
    "    return stripped\n",
    "\n",
    "\n",
    "def extract_link_sentences(html: str,nlp,count,label: str = \"REFERENCE\"):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    container = soup.find(\"div\", {\"class\": \"tekst\"})\n",
    "    if not container:\n",
    "        return []\n",
    "\n",
    "    text = container.get_text(\" \")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sentences]\n",
    "\n",
    "    # ---- collect valid link texts ----\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", {\"class\": \"popuplink\"}):\n",
    "        link_text = clean_link_text(a.get_text(strip=True))\n",
    "        if link_text:\n",
    "            links.append(link_text)\n",
    "\n",
    "    if not links:\n",
    "        return [],[]\n",
    "    output_ref = []\n",
    "    output_not = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        raw_entities = []\n",
    "\n",
    "        # ---- collect all possible matches first ----\n",
    "        pos=[]\n",
    "        i=0\n",
    "        search_pos = 0  \n",
    "        for link in links:\n",
    "            # exact matches\n",
    "            pattern = re.compile(re.escape(link))\n",
    "            match = pattern.search(sent, search_pos)\n",
    "            if match:\n",
    "                raw_entities.append((match.start(), match.end(), label))\n",
    "                search_pos = match.end()  \n",
    "                pos.append(i)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        if not raw_entities:\n",
    "            output_not.append((sent, {\"entities\": []},count))\n",
    "            continue\n",
    "\n",
    "        # ---- enforce ordering rule ----\n",
    "        raw_entities.sort(key=lambda x: x[0])  # sort by start offset\n",
    "\n",
    "        filtered_entities = []\n",
    "        last_start = -1\n",
    "        k=0\n",
    "        d=0\n",
    "        for start, end, lbl in raw_entities:\n",
    "            if start > last_start:\n",
    "                filtered_entities.append((start, end, lbl))\n",
    "                last_start = start\n",
    "                links.pop(pos[k]-d)\n",
    "                d+=1\n",
    "            k+=1\n",
    "            # else: ignore match (starts before or at same position)\n",
    "\n",
    "        if filtered_entities:\n",
    "            output_ref.append((sent, {\"entities\": filtered_entities},count))\n",
    "    return output_ref,output_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64567871-083a-40aa-9ba6-495e066c5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need the html text\n",
    "texts=link_tex.values()\n",
    "#all found references\n",
    "training_data_ref=[]\n",
    "#all sentences without refrences\n",
    "training_data_neg=[]\n",
    "count=0\n",
    "\n",
    "for text in texts:\n",
    "    result_ref,result_neg=extract_link_sentences(text,nlp,count)\n",
    "    if result_ref != []:\n",
    "        training_data_ref.extend(result_ref)\n",
    "        training_data_neg.extend(result_neg)\n",
    "    else:\n",
    "        print(\"empty\")\n",
    "    count+=1\n",
    "    if count%500==0:\n",
    "        print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "237f01ab-7738-4389-86dd-db260a034864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open(\"training_data.pkl\", \"wb\") as b:\n",
    "        pickle.dump(training_data_ref, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "423a77a8-588c-45d1-a7a3-589ff4c5e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data…\n",
      "Shuffling and splitting…\n",
      "Train size: 1197\n",
      "Dev size:   133\n",
      "Loading spaCy tokenizer…\n",
      "Converting training data → train.spacy…\n",
      "Converting dev data → dev.spacy…\n",
      "\n",
      "Done!\n",
      "Saved: train.spacy, dev.spacy\n"
     ]
    }
   ],
   "source": [
    "#data spacy\n",
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "\n",
    "INPUT_FILE = \"training_data.pkl\"\n",
    "TRAIN_OUT = \"train.spacy\"\n",
    "DEV_OUT = \"dev.spacy\"\n",
    "DEV_SPLIT = 0.1      \n",
    "NLP_MODEL = \"nl_core_news_sm\"   # tokenizer\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def create_docbin(nlp, data):\n",
    "    db = DocBin()\n",
    "    for text, ann,_ in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in ann[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data…\")\n",
    "    data = load_data(INPUT_FILE)\n",
    "\n",
    "    print(\"Shuffling and splitting…\")\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * (1 - DEV_SPLIT))\n",
    "    train_data = data[:split]\n",
    "    dev_data = data[split:]\n",
    "\n",
    "    print(f\"Train size: {len(train_data)}\")\n",
    "    print(f\"Dev size:   {len(dev_data)}\")\n",
    "\n",
    "    print(\"Loading spaCy tokenizer…\")\n",
    "    nlp = spacy.load(NLP_MODEL)\n",
    "\n",
    "    print(\"Converting training data → train.spacy…\")\n",
    "    train_db = create_docbin(nlp, train_data)\n",
    "    train_db.to_disk(TRAIN_OUT)\n",
    "\n",
    "    print(\"Converting dev data → dev.spacy…\")\n",
    "    dev_db = create_docbin(nlp, dev_data)\n",
    "    dev_db.to_disk(DEV_OUT)\n",
    "\n",
    "    print(\"\\nDone!\")\n",
    "    print(f\"Saved: {TRAIN_OUT}, {DEV_OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9e13-56f8-4ff0-942c-bb53ea801c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data logic\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "LABEL = \"REFERENCE\"\n",
    "\n",
    "\n",
    "\n",
    "def sentence_has_reference(ann: Dict[str, Any], label_name: str = LABEL) -> int:\n",
    "    ents = ann.get(\"entities\", [])\n",
    "    return int(any(lab == label_name for (_, _, lab) in ents))\n",
    "\n",
    "\n",
    "\n",
    "#  Build X and y\n",
    "examples=training_data_ref+training_data_neg\n",
    "X_sent = [text for (text, _, _) in examples]\n",
    "\n",
    "y_sent = np.array([sentence_has_reference(ann) for (_, ann, _) in examples], dtype=np.int32)\n",
    "doc_ids = np.array([doc_id for (_, _, doc_id) in examples], dtype=object)\n",
    "\n",
    "# recombines the data to the orginal files\n",
    "unique_docs = np.unique(doc_ids)\n",
    "train_docs, test_docs = train_test_split(unique_docs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_mask = np.isin(doc_ids, train_docs)\n",
    "test_mask  = np.isin(doc_ids, test_docs)\n",
    "\n",
    "X_train = [X_sent[i] for i in np.where(train_mask)[0]]\n",
    "y_train = y_sent[train_mask]\n",
    "\n",
    "X_test  = [X_sent[i] for i in np.where(test_mask)[0]]\n",
    "y_test  = y_sent[test_mask]\n",
    "\n",
    "\n",
    "with open(\"LG_x_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(X_train, b)\n",
    "with open(\"LG_y_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(y_train, b)\n",
    "with open(\"LG_x_test.pkl\", \"wb\") as b:\n",
    "    pickle.dump(X_test, b)\n",
    "with open(\"LG_y_test.pkl\", \"wb\") as b:\n",
    "    pickle.dump(y_test, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "006367a9-c5b2-4bf1-b23e-2800b2e19e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1197\n",
      "Eval size: 133\n"
     ]
    }
   ],
   "source": [
    "#data roberta\n",
    "import pickle\n",
    "with open(\"training_data.pkl\", \"rb\") as f:\n",
    "    TRAIN_DATA=pickle.load(f)\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def preprocess_spacy_dataset(dataset, tokenizer, label2id, max_length=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for text, ann,_ in dataset:\n",
    "        entities = ann[\"entities\"]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        token_labels = [\"O\"] * len(offsets)\n",
    "\n",
    "        # Assign BIO labels\n",
    "        for start, end, ent_label in entities:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == tok_end:\n",
    "                    continue  # special tokens\n",
    "\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tok_start == start:\n",
    "                        token_labels[i] = f\"B-{ent_label}\"\n",
    "                    else:\n",
    "                        token_labels[i] = f\"I-{ent_label}\"\n",
    "\n",
    "        # Convert to label IDs\n",
    "        label_ids = []\n",
    "        for lab, (tok_start, tok_end) in zip(token_labels, offsets):\n",
    "            if tok_start == tok_end:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[lab])\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])\n",
    "        attention_masks.append(encoding[\"attention_mask\"])\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "LABEL_LIST = [\"O\", \"B-REFERENCE\", \"I-REFERENCE\"]\n",
    "label2id = {l: i for i, l in enumerate(LABEL_LIST)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "MODEL_NAME = \"pdelobelle/robbert-v2-dutch-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "processed = preprocess_spacy_dataset(\n",
    "    TRAIN_DATA,\n",
    "    tokenizer,\n",
    "    label2id,\n",
    "    max_length=256\n",
    ")\n",
    "dataset = Dataset.from_dict(processed)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "with open(\"roberta_train.pkl\", \"wb\") as b:\n",
    "        pickle.dump(train_dataset, b)\n",
    "with open(\"roberta_test.pkl\", \"wb\") as b:\n",
    "        pickle.dump(eval_dataset, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d0df6-cf80-43db-abb8-ea7be98029cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
