{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5106db30-38bb-4317-b290-a452fbc672c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed drivers\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa0578f-fe9d-45aa-b8f5-7c2fc61e3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to continue your scraping\n",
    "#database with all used links\n",
    "#(link:used)\n",
    "with open(\"link_used.pkl\", \"rb\") as f:\n",
    "        link_used=pickle.load(f)\n",
    "# all obtained text in html and their links \n",
    "# #(link:text)\n",
    "link_tex= {}\n",
    "# with open(\"link_tex_eval.pkl\", \"rb\") as f:\n",
    "#         link_tex=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022134c2-a2b7-41c4-9c6b-a34c4d8ed0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url,link_used,link_tex):\n",
    "    #settings for the websdriver\n",
    "    options = webdriver.ChromeOptions() \n",
    "    # dowload location\n",
    "    options.add_argument(\"--headless\") \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    driver.refresh()\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    \n",
    "    # step 2 find text\n",
    "    text=soup.find(\"div\", {\"class\": \"tekst\"})\n",
    "    \n",
    "    if not text:\n",
    "        link_used[url]=\"Y\"\n",
    "        return link_used,link_tex\n",
    "    else:\n",
    "        #safes link and their text in to the database\n",
    "        link_tex[url]=str(text)\n",
    "        # step 3 find links\n",
    "        links=text.find_all(\"a\", {\"class\": \"popuplink\"})\n",
    "        for a_tag in links:\n",
    "            #reference text\n",
    "            link_text = a_tag.get_text(strip=True)\n",
    "            # link to other court ruling or law\n",
    "            # Skip empty or whitespace-only links\n",
    "            if not link_text or link_text.isspace():\n",
    "                continue\n",
    "        \n",
    "            # step 4 add found links to dict with links\n",
    "            #make the found link in to a usable link\n",
    "            trans=transform_url(a_tag['href'])\n",
    "            #checks if not already in the dict\n",
    "            if trans not in link_used:\n",
    "                link_used[trans]=\"N\"\n",
    "\n",
    "    # step 9 set current link as searched .\n",
    "    link_used[url]=\"Y\"\n",
    "    driver.close()\n",
    "    return link_used,link_tex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad17483-5b2f-4bdd-9949-9d5746440ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the unicode link to a usable link\n",
    "def transform_url(url: str) -> str:\n",
    "    # Extract the id=... part\n",
    "    m = re.search(r'id=[^&]+', url)\n",
    "    if not m:\n",
    "        raise ValueError(\"No 'id=' parameter found\")\n",
    "\n",
    "    id_param = m.group(0)  # e.g. 'id=http%3A%2F%2F...BJ6879'\n",
    "\n",
    "    # Build the new URL\n",
    "    new_url = (\n",
    "        f\"https://linkeddata.overheid.nl/front/portal/document-viewer?\"\n",
    "        f\"{id_param}&callback=&dates=&fields=\"\n",
    "    )\n",
    "    return new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba8c253-a8ed-4d49-81b4-e0dcfed097d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chooses a new link from the database\n",
    "def new_url(link_used):\n",
    "    x = \"N\"\n",
    "    first_key=\"empty\"\n",
    "    #check if unused links\n",
    "    for k, v in link_used.items():\n",
    "        if v == x:\n",
    "            first_key = k\n",
    "            break\n",
    "    if first_key==\"empty\":\n",
    "        link_used,first_key=random_new_url(link_used)\n",
    "        print(first_key)\n",
    "\n",
    "        \n",
    "    return link_used,transform_url(first_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbe912-e242-4bda-ab6c-175a8a165123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size is: 67 2\n"
     ]
    }
   ],
   "source": [
    "# picks a url to start\n",
    "link_used,url=new_url(link_used)\n",
    "#choose this number as a minum set size\n",
    "saves=10\n",
    "max_number=10\n",
    "for z in range(0,max_number):\n",
    "    for x in range(0,saves):\n",
    "        link_used,link_tex = get_data(url,link_used,link_tex)\n",
    "        #\n",
    "        link_used,url=new_url(link_used)\n",
    "    print(\"size is:\" ,len(link_used),len(link_tex))\n",
    "    with open(\"link_used.pkl\", \"wb\") as f:\n",
    "        pickle.dump(link_used, f)\n",
    "    with open(\"link_tex_eva.pkl\", \"wb\") as b:\n",
    "        pickle.dump(link_tex, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08812ba8-2c36-4a77-b9f2-ea7fcf45bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_tex_eval.pkl\", \"rb\") as b:\n",
    "        link_tex=pickle.load(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec016da-949f-4e3b-b37e-04abed79b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_link_text(t):\n",
    "    \"\"\"Return cleaned link text or None if invalid.\"\"\"\n",
    "    if t is None:\n",
    "        return None\n",
    "    stripped = t.strip()\n",
    "    if stripped == \"\" or all(ch in string.punctuation for ch in stripped):\n",
    "        return None\n",
    "    return stripped\n",
    "\n",
    "\n",
    "def extract_link_sentences(html: str,nlp,count,label: str = \"REFERENCE\"):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    container = soup.find(\"div\", {\"class\": \"tekst\"})\n",
    "    if not container:\n",
    "        return []\n",
    "\n",
    "    text = container.get_text(\" \")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sentences]\n",
    "\n",
    "    # ---- collect valid link texts ----\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", {\"class\": \"popuplink\"}):\n",
    "        link_text = clean_link_text(a.get_text(strip=True))\n",
    "        if link_text:\n",
    "            links.append(link_text)\n",
    "\n",
    "    if not links:\n",
    "        return [],[]\n",
    "    output_ref = []\n",
    "    output_not = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        raw_entities = []\n",
    "\n",
    "        # ---- collect all possible matches first ----\n",
    "        pos=[]\n",
    "        i=0\n",
    "        search_pos = 0  \n",
    "        for link in links:\n",
    "            # exact matches\n",
    "            pattern = re.compile(re.escape(link))\n",
    "            match = pattern.search(sent, search_pos)\n",
    "            if match:\n",
    "                raw_entities.append((match.start(), match.end(), label))\n",
    "                search_pos = match.end()  \n",
    "                pos.append(i)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        if not raw_entities:\n",
    "            output_not.append((sent, {\"entities\": []},count))\n",
    "            continue\n",
    "\n",
    "        # ---- enforce ordering rule ----\n",
    "        raw_entities.sort(key=lambda x: x[0])  # sort by start offset\n",
    "\n",
    "        filtered_entities = []\n",
    "        last_start = -1\n",
    "        k=0\n",
    "        d=0\n",
    "        for start, end, lbl in raw_entities:\n",
    "            if start > last_start:\n",
    "                filtered_entities.append((start, end, lbl))\n",
    "                last_start = start\n",
    "                links.pop(pos[k]-d)\n",
    "                d+=1\n",
    "            k+=1\n",
    "            # else: ignore match (starts before or at same position)\n",
    "\n",
    "        if filtered_entities:\n",
    "            output_ref.append((sent, {\"entities\": filtered_entities},count))\n",
    "    return output_ref,output_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dd820-702e-4b74-a4aa-4f07665ef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need the html text\n",
    "texts=link_tex.values()\n",
    "#all found references\n",
    "training_data_ref=[]\n",
    "#all sentences without refrences\n",
    "training_data_neg=[]\n",
    "count=0\n",
    "\n",
    "for text in texts:\n",
    "    result_ref,result_neg=extract_link_sentences(text,nlp,count)\n",
    "    if result_ref != []:\n",
    "        training_data_ref.extend(result_ref)\n",
    "        training_data_neg.extend(result_neg)\n",
    "    else:\n",
    "        print(\"empty\")\n",
    "    count+=1\n",
    "    if count%500==0:\n",
    "        print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba2a86-09cf-42a7-b6e8-0600ae3c5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=training_data_ref+training_data_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0d9aa-d9ce-4804-8388-6965aef34dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"eval_data.pkl\", \"wb\") as b:\n",
    "        pickle.dump(examples, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
